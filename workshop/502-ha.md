# 502: Scaling and High Availability

In the slides we talked about two things.

- When, how and why to scale out Prometheus
- How to make Prometheus and Alertmanager highly available

In this workshop we're going to pretend that we have two teams and we want to setup federation.

Also we're going to look at a HA mode. This is a little contrived, as you wouldn't normally be
running a HA setup in a single cluster. I.e. K8s is already pretty resilient; it will restart your
alertmanager pod if it fails and send the alert when it starts.

In other words, if you do this for real, make sure your second alertmanager instance is separated
from your cluster. Otherwise if your cluster went down, you'd lose both instances.

## Federation

I'm not going to provide too much help on this one, because you should find it relatively easy.
There's only a few new lines of configuration. The rest is copy/pasting from previous exercises.

Task:

- Create two instances of Prometheus (it's up to you how you do this. Separate namespaces, just
  another deployment and service, whatever.
- Create a third instance and federate the the first two.

So in other words create two Prometheus instances just like you have been doing before. Then when
creating the federating Prometheus, you'll want to alter the settings slightly to federate the first
two (and not do any other scraping):

```yaml
- scrape_config:
  - job_name: federated_prometheus
    honor_labels: true
    metrics_path: /federate         # Default path to get data from other Prom nodes
    params:
      match[]:
        - '{__name__=~"^job:.*"}'   # Request all job-level time series
    static_configs:
      - targets:
        - dc1-prometheus:9090
        - dc2-prometheus:9090
```

For the targets you could either use the service DNS addresses or you can use the Kubernetes service
discovery definition.

## High Availability

This one is a bit more tricky because of the DNS requirements of the Alertmanager.

The best way I thought of doing it in this environment was to create two separate Alertmanager
deployment and services, then use the service name DNS entry.

Tasks:
- Take the manifest from `501-alerting/manifest-alerting.yml` and replicate the alertmanager
  deployment specification and service specification.
- Add
- Add the `mesh.peer` arguments to each of the deployments (see below)
- Add port `6783` to each of the services and deployments (this is the Gossip protocol port)

```yaml
args:
  - '-log.level=debug'
  - '-config.file=/etc/alertmanager/config.yml'
  - '-mesh.peer=alertmanager-1.monitoring.svc.cluster.local:6783'
  - '-mesh.peer=alertmanager-2.monitoring.svc.cluster.local:6783'
```

If you get stuck, ask for help. Or check out the version I implemented in `502-ha/manifest.yml`.
